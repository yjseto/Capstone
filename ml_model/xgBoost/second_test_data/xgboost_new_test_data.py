# -*- coding: utf-8 -*-
"""xgBoost_new_test_data.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HA_yHv6mD6I4MjYmFsYSmHWvMY96S-zA
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from xgboost import XGBClassifier
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
from imblearn.over_sampling import SMOTE
from sklearn.feature_selection import SelectFromModel

# Load the dataset
df = pd.read_csv('new_test_data_oct_7.csv')

# Handle missing values
df = df.dropna()

# Function to map original severity to new categories
def map_severity(severity):
    if severity in ['No Apparent Injury']:
        return 0  # No/Minor Injury
    elif severity in ['Suspected Minor Injury', 'Possible Injury']:
        return 1  # Moderate Injury
    else:  # 'Suspected Serious Injury', 'Fatal Injury (Killed)'
        return 2  # Severe/Fatal Injury

# Filter and create new severity column
df_filtered = df[~df['Crash_Severity'].isin(['Unknown', 'Died Prior to Crash'])].copy()
df_filtered['severity_grouped'] = df_filtered['Crash_Severity'].map(map_severity)

# Function to handle mixed data types
def encode_mixed_types(df, col):
    if df[col].dtype == 'object':
        return LabelEncoder().fit_transform(df[col].astype(str))
    else:
        return df[col]

# Encode all columns
for col in df_filtered.columns:
    df_filtered[col] = encode_mixed_types(df_filtered, col)

# List of features to exclude (severity-related and identifier features)
exclude_features = ['Crash_Number', 'Crash_Severity', 'severity_grouped', 'serious_with_fatalities',
                    'fatalities', 'serious', 'minor', 'area']

print(f"Number of features before selection: {X.shape[1]}")


# Prepare features and target variable
X = df_filtered.drop(exclude_features, axis=1)
y = df_filtered['severity_grouped']

# Feature selection
# selector = SelectFromModel(XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42))
# selector.fit(X, y)
# X_selected = selector.transform(X)
# selected_feat = X.columns[selector.get_support()]

# For now, use all features
X_selected = X
selected_feat = X.columns

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)

# Apply SMOTE
smote = SMOTE(random_state=42)
X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)

# Create and train the XGBoost model
model = XGBClassifier(
    max_depth=7,
    min_child_weight=3,
    gamma=0,
    subsample=1.0,
    colsample_bytree=1.0,
    learning_rate=0.1,
    n_estimators=200,
    use_label_encoder=False,
    eval_metric='mlogloss',
    random_state=42
)

# Fit the model
model.fit(X_train_smote, y_train_smote)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Print the classification report and confusion matrix
print("\nClassification Report:")
print(classification_report(y_test, y_pred))
print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred))

# Get feature importances
feat_importances = pd.Series(model.feature_importances_, index=selected_feat)

# Sort features by importance
sorted_importances = feat_importances.sort_values(ascending=False)

# Get the number of features to display (all if less than 10, otherwise 10)
n_features_to_display = min(len(sorted_importances), 10)

# Plot feature importance
plt.figure(figsize=(10, 6))
sorted_importances[:n_features_to_display].plot(kind='barh')
plt.title(f"Top {n_features_to_display} Contributing Factors to Car Crash Severity")
plt.xlabel("Feature Importance Score")
plt.tight_layout()
plt.show()

# Print insights
print("\nInsights:")
print(f"The top {n_features_to_display} contributing factors to car crash severity in Alaska are:")
for i, (feature, importance) in enumerate(sorted_importances[:n_features_to_display].items(), 1):
    print(f"   {i}. {feature} (Importance: {importance:.4f})")

print("\n2. Model performance and feature importance can be seen in the outputs above.")
print("3. Further analysis may be needed to understand the specific impact of each factor on crash severity.")

# Print total number of features used in the model
print(f"\nTotal number of features used in the model: {len(sorted_importances)}")

# Optionally, print all features and their importances
print("\nAll features and their importance scores:")
print(sorted_importances)