# -*- coding: utf-8 -*-
"""xgBoost_new_test_data.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HA_yHv6mD6I4MjYmFsYSmHWvMY96S-zA
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from xgboost import XGBClassifier
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
from imblearn.over_sampling import SMOTE

# Load the dataset
df = pd.read_csv('new_test_data_oct_7.csv', low_memory=False)

# Handle missing values
df = df.dropna()

# Function to map original severity to new categories
def map_severity(severity):
    if severity in ['No Apparent Injury']:
        return 0  # No Injury
    elif severity in ['Suspected Minor Injury', 'Possible Injury']:
        return 1  # Moderate Injury
    else:  # 'Suspected Serious Injury', 'Fatal Injury (Killed)'
        return 2  # Severe/Fatal Injury

# Filter and create new severity column
df_filtered = df[~df['Crash_Severity'].isin(['Unknown', 'Died Prior to Crash'])].copy()
df_filtered['severity_grouped'] = df_filtered['Crash_Severity'].map(map_severity)

# Print column names to verify
print("Columns in the dataset:", df_filtered.columns.tolist())

# Convert Number_of_Motorized_units to numeric, replacing non-numeric values with NaN
df_filtered['Number_of_Motorized_units'] = pd.to_numeric(df_filtered['Number_of_Motorized_units'], errors='coerce')

# Fill NaN values with 1 (assuming a single unit for unknown cases)
df_filtered['Number_of_Motorized_units'] = df_filtered['Number_of_Motorized_units'].fillna(1)

# Now perform the adjustment
df_filtered['Additional_Motorized_Units'] = df_filtered['Number_of_Motorized_units'] - 1

# Ensure the result is non-negative
df_filtered['Additional_Motorized_Units'] = df_filtered['Additional_Motorized_Units'].clip(lower=0)

# Function to handle mixed data types
def encode_mixed_types(df, col):
    if df[col].dtype == 'object':
        return LabelEncoder().fit_transform(df[col].astype(str))
    else:
        return df[col]

# Encode all columns
for col in df_filtered.columns:
    df_filtered[col] = encode_mixed_types(df_filtered, col)

# List of features to exclude (severity-related and identifier features)
exclude_features = ['Crash_Number', 'Crash_Severity', 'severity_grouped', 'serious_with_fatalities',
                    'fatalities', 'serious', 'minor', 'area', 'pop10', 'Number_of_Motorized_units']

# Prepare features and target variable
X = df_filtered.drop(exclude_features, axis=1)
y = df_filtered['severity_grouped']

print(f"Number of features: {X.shape[1]}")

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Apply SMOTE
smote = SMOTE(random_state=42)
X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)

# Create and train the XGBoost model
model = XGBClassifier(
    max_depth=7,
    min_child_weight=3,
    gamma=0,
    subsample=1.0,
    colsample_bytree=1.0,
    learning_rate=0.1,
    n_estimators=200,
    use_label_encoder=False,
    eval_metric='mlogloss',
    random_state=42
)

# Fit the model
model.fit(X_train_smote, y_train_smote)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Print the classification report and confusion matrix
print("\nClassification Report:")
print(classification_report(y_test, y_pred))
print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred))

# Get feature importances
importances = model.feature_importances_
feature_importances = pd.DataFrame({'feature': X.columns, 'importance': importances})
feature_importances = feature_importances.sort_values('importance', ascending=False)

# Function to plot and print top features
def plot_top_features(importances, title):
    plt.figure(figsize=(10, 6))
    importances.head(10).plot(x='feature', y='importance', kind='barh')
    plt.title(title)
    plt.xlabel("Feature Importance Score")
    plt.tight_layout()
    plt.show()

    print(f"\nTop 10 contributing factors for {title}:")
    for i, (_, row) in enumerate(importances.head(10).iterrows(), 1):
        print(f"   {i}. {row['feature']} (Importance: {row['importance']:.4f})")

# Plot and print overall feature importance
plot_top_features(feature_importances, "Overall Feature Importance")

# Get feature importance for each class
class_importances = []
for i in range(3):
    class_pred = (y_train_smote == i).astype(int)
    class_model = XGBClassifier(
        max_depth=7, min_child_weight=3, gamma=0,
        subsample=1.0, colsample_bytree=1.0,
        learning_rate=0.1, n_estimators=200,
        use_label_encoder=False, eval_metric='logloss',
        random_state=42
    )
    class_model.fit(X_train_smote, class_pred)
    class_importance = pd.DataFrame({'feature': X.columns, 'importance': class_model.feature_importances_})
    class_importance = class_importance.sort_values('importance', ascending=False)
    class_importances.append(class_importance)

severity_levels = [0, 1, 2]
severity_names = ['No/Minor Injury', 'Moderate Injury', 'Severe/Fatal Injury']

for i, level in enumerate(severity_levels):
    plot_top_features(class_importances[i], f"{severity_names[i]} (Class {level})")

# Create a heatmap to compare feature importance across severity levels
top_features = set()
for imp in class_importances:
    top_features.update(imp['feature'].head(10))

heatmap_data = pd.DataFrame({severity_names[i]: imp.set_index('feature')['importance']
                             for i, imp in enumerate(class_importances)}, index=list(top_features)).fillna(0)

plt.figure(figsize=(12, 10))
sns.heatmap(heatmap_data, annot=True, cmap='YlOrRd', fmt='.2f')
plt.title("Feature Importance Heatmap Across Severity Levels")
plt.show()