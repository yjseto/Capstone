# -*- coding: utf-8 -*-
"""multilevel_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CYiBaZC3A9rIr0eqV-qlTI6I9Iqq8Mf5
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from statsmodels.formula.api import mixedlm
import statsmodels.api as sm
import matplotlib.pyplot as plt
import seaborn as sns
import re

# Load the dataset
df = pd.read_csv('new_test_data_oct_7.csv', low_memory=False)

# Handle missing values
df = df.dropna()

def map_severity(severity):
    if severity in ['No Apparent Injury']:
        return 0  # No Injury
    elif severity in ['Suspected Minor Injury', 'Possible Injury']:
        return 1  # Moderate Injury
    elif severity in ['Suspected Serious Injury', 'Fatal Injury (Killed)']:
        return 2  # Severe/Fatal Injury
    else:
        print(f"Unexpected severity value: {severity}")
        return -1

# Filter and create new severity column
df_filtered = df[~df['Crash_Severity'].isin(['Unknown', 'Died Prior to Crash'])].copy()
df_filtered['severity_grouped'] = df_filtered['Crash_Severity'].map(map_severity)

# Handle Additional_Motorized_Units
df_filtered['Number_of_Motorized_units'] = pd.to_numeric(df_filtered['Number_of_Motorized_units'], errors='coerce')
df_filtered['Number_of_Motorized_units'] = df_filtered['Number_of_Motorized_units'].fillna(1)
df_filtered['Additional_Motorized_Units'] = (df_filtered['Number_of_Motorized_units'] - 1).clip(lower=0)

# List of features to exclude
exclude_features = ['Crash_Number', 'Crash_Severity', 'severity_grouped', 'serious_with_fatalities',
                    'fatalities', 'serious', 'minor', 'pop10', 'Number_of_Motorized_units', 'area', 'Airbags']

# Prepare features and target variable
features = [col for col in df_filtered.columns if col not in exclude_features]

# Function to clean column names
def clean_column_name(name):
    # Remove any characters that are not letters, numbers, or underscores
    name = re.sub(r'[^\w]', '_', name)
    # Ensure the name doesn't start with a number
    if name[0].isdigit():
        name = 'col_' + name
    return name

# Encode categorical variables and clean column names
le = LabelEncoder()
for feature in features:
    if df_filtered[feature].dtype == 'object':
        df_filtered[clean_column_name(f'{feature}_encoded')] = le.fit_transform(df_filtered[feature].astype(str))
    else:
        df_filtered[clean_column_name(f'{feature}_encoded')] = df_filtered[feature]

# Prepare the data for the model
model_features = [clean_column_name(f'{feature}_encoded') for feature in features]
model_data = df_filtered[model_features + ['severity_grouped', 'area']]

# Split the data into training and testing sets
X = model_data[model_features]
y = model_data['severity_grouped']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Combine training data
train_data = pd.concat([X_train, pd.Series(y_train, name='severity_grouped')], axis=1)
train_data['area'] = df_filtered.loc[train_data.index, 'area']

# Prepare the formula for the model
formula = "severity_grouped ~ " + " + ".join(model_features)

# Fit the multilevel model
model = mixedlm(formula, data=train_data, groups="area")
result = model.fit()

# Print model summary
print(result.summary())

# Make predictions on the test set
X_test_with_area = X_test.copy()
X_test_with_area['area'] = df_filtered.loc[X_test.index, 'area']
predictions = result.predict(X_test_with_area)

# Convert predictions to severity levels
predicted_severity = np.round(predictions).astype(int).clip(0, 2)

# Calculate accuracy
accuracy = np.mean(predicted_severity == y_test)
print(f"Model accuracy: {accuracy:.2f}")

# Analyze severity levels by area
severity_by_area = pd.crosstab(df_filtered['area'], df_filtered['severity_grouped'], normalize='index')
print("\nSeverity levels by area (proportions):")
print(severity_by_area)

# Visualize severity levels by area
plt.figure(figsize=(12, 6))
severity_by_area.plot(kind='bar', stacked=True)
plt.title('Crash Severity Levels by Area')
plt.xlabel('Area')
plt.ylabel('Proportion')
plt.legend(title='Severity Level', labels=['No Injury', 'Moderate Injury', 'Severe/Fatal Injury'])
plt.tight_layout()
plt.show()

# Visualize the results
plt.figure(figsize=(10, 6))
sns.boxplot(x='area', y='severity_grouped', data=df_filtered)
plt.title('Crash Severity by Area')
plt.show()

# Feature importance
feature_importance = pd.Series(result.params[1:], index=model_features).abs().sort_values(ascending=False)
plt.figure(figsize=(12, 6))
feature_importance.plot(kind='bar')
plt.title('Feature Importance')
plt.xticks(rotation=90)
plt.tight_layout()
plt.show()

# Analyze model predictions by area
predictions_df = pd.DataFrame({
    'true_severity': y_test,
    'predicted_severity': predicted_severity,
    'area': df_filtered.loc[y_test.index, 'area']
})

for area in predictions_df['area'].unique():
    area_predictions = predictions_df[predictions_df['area'] == area]
    print(f"\nConfusion Matrix for {area}:")
    print(pd.crosstab(area_predictions['true_severity'], area_predictions['predicted_severity'],
                      rownames=['Actual'], colnames=['Predicted']))
    area_accuracy = np.mean(area_predictions['true_severity'] == area_predictions['predicted_severity'])
    print(f"Accuracy for {area}: {area_accuracy:.2f}")

# Visualize model performance by area
plt.figure(figsize=(12, 6))
sns.heatmap(pd.crosstab(predictions_df['area'], predictions_df['true_severity'] == predictions_df['predicted_severity'],
                        normalize='index'),
            annot=True, fmt='.2f', cmap='YlGnBu')
plt.title('Model Accuracy by Area')
plt.xlabel('Correct Prediction')
plt.ylabel('Area')
plt.tight_layout()
plt.show()


# Feature importance for each severity level at each area
def get_feature_importance(X, y):
    rf = RandomForestClassifier(n_estimators=100, random_state=42)
    rf.fit(X, y)
    return pd.Series(rf.feature_importances_, index=X.columns)

# Prepare data for feature importance analysis
X_full = model_data[model_features]
y_full = model_data['severity_grouped']
area_full = model_data['area']

# Create a dictionary to store feature importances
feature_importances = {}

# Calculate feature importance for each severity level at each area
for area in area_full.unique():
    feature_importances[area] = {}
    area_mask = area_full == area
    X_area = X_full[area_mask]
    y_area = y_full[area_mask]

    for severity in y_area.unique():
        severity_mask = y_area == severity
        X_severity = X_area[severity_mask]
        y_severity = y_area[severity_mask]

        if len(y_severity) > 0:  # Check if there are samples for this severity level
            importance = get_feature_importance(X_severity, y_severity)
            feature_importances[area][severity] = importance

# Visualize feature importance for each severity level at each area
severity_labels = ['No Injury', 'Moderate Injury', 'Severe/Fatal Injury']
for area in feature_importances:
    fig, axes = plt.subplots(1, 3, figsize=(20, 6))
    fig.suptitle(f'Feature Importance by Severity Level in {area}')

    for severity, ax in zip(range(3), axes):
        if severity in feature_importances[area]:
            importance = feature_importances[area][severity].sort_values(ascending=False)
            importance.plot(kind='bar', ax=ax)
            ax.set_title(f'Severity: {severity_labels[severity]}')
            ax.set_xticklabels(ax.get_xticklabels(), rotation=90)
            ax.set_ylabel('Importance')
        else:
            ax.text(0.5, 0.5, 'No data for this severity level',
                    horizontalalignment='center', verticalalignment='center')

    plt.tight_layout()
    plt.show()

# Calculate and visualize average feature importance across all areas and severity levels
all_importances = []
for area in feature_importances:
    for severity in feature_importances[area]:
        all_importances.append(feature_importances[area][severity])

avg_importance = pd.DataFrame(all_importances).mean()
avg_importance_sorted = avg_importance.sort_values(ascending=False)

plt.figure(figsize=(12, 6))
avg_importance_sorted.plot(kind='bar')
plt.title('Average Feature Importance Across All Areas and Severity Levels')
plt.xlabel('Features')
plt.ylabel('Average Importance')
plt.xticks(rotation=90)
plt.tight_layout()
plt.show()

# Print top 5 most important features for each area and severity level
for area in feature_importances:
    print(f"\nTop 5 important features for {area}:")
    for severity in feature_importances[area]:
        importance = feature_importances[area][severity].sort_values(ascending=False)
        print(f"  Severity {severity} ({severity_labels[severity]}):")
        print(importance.head().to_string())
        print()