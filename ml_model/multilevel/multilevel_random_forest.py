# -*- coding: utf-8 -*-
"""multilevel_random_forest.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1j5DBM78LPJvVFWVOKHOIFZSErUo2RVvN
"""

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.preprocessing import RobustScaler
from sklearn.model_selection import KFold
from sklearn.feature_selection import SelectKBest, f_classif

def prepare_predictors(df, selected_features=None):
    """
    Prepare predictors using the sam   preprocessing as original code
    """
    df = df.copy()

    exclude_features = ['Crash_Number', 'Crash_Severity', 'severity_grouped',
                       'serious_with_fatalities', 'fatalities', 'serious',
                       'minor', 'pop10', 'Number_of_Motorized_units',
                       'Airbags', 'Unnamed: 0', 'severity_score',
                       'is_no_injury', 'is_moderate', 'is_severe', 'Causual_Unit_First']

    categorical_columns = []
    numeric_columns = []

    for col in df.columns:
        if col not in exclude_features and col != 'area':
            if df[col].dtype == 'object':
                categorical_columns.append(col)
            elif df[col].dtype in ['int64', 'float64']:
                numeric_columns.append(col)

    X = pd.DataFrame(index=df.index)

    # Handle numeric columns with robust scaling
    if numeric_columns:
        X_numeric = df[numeric_columns].copy()
        scaler = RobustScaler()
        X_numeric = pd.DataFrame(scaler.fit_transform(X_numeric),
                               columns=numeric_columns,
                               index=df.index)
        X = pd.concat([X, X_numeric], axis=1)

    # Frequency encoding for categorical variables
    for col in categorical_columns:
        df[col] = df[col].fillna('missing')
        freq_encoding = df[col].value_counts(normalize=True)
        X[f'{col}_freq'] = df[col].map(freq_encoding)

    if selected_features is not None:
        X = X[selected_features]

    return X

class HierarchicalRandomForest:
    def __init__(self, n_estimators=100, max_depth=None):
        self.global_rf = RandomForestClassifier(
            n_estimators=n_estimators,
            max_depth=max_depth,
            random_state=42,
            class_weight='balanced'
        )
        self.local_rfs = {}
        self.feature_importances_ = None
        self.group_performances = {}

    def fit(self, X, y, groups):
        # Fit global model
        self.global_rf.fit(X, y)
        global_preds = self.global_rf.predict_proba(X)[:, 1]

        # Fit local models for each group
        unique_groups = np.unique(groups)
        for group in unique_groups:
            mask = (groups == group)
            if sum(mask) >= 50:  # Minimum samples threshold
                local_rf = RandomForestClassifier(
                    n_estimators=50,  # Smaller forest for local models
                    max_depth=5,
                    random_state=42,
                    class_weight='balanced'
                )
                local_rf.fit(X[mask], y[mask])
                self.local_rfs[group] = local_rf

                # Calculate local model performance
                local_preds = local_rf.predict_proba(X[mask])[:, 1]
                self.group_performances[group] = {
                    'samples': sum(mask),
                    'accuracy': accuracy_score(y[mask], local_preds > 0.5),
                    'auc': roc_auc_score(y[mask], local_preds)
                }

        self.feature_importances_ = self.global_rf.feature_importances_
        return self

    def predict_proba(self, X, groups):
        predictions = np.zeros((X.shape[0], 2))
        global_preds = self.global_rf.predict_proba(X)

        # Combine global and local predictions
        for i, group in enumerate(groups):
            if group in self.local_rfs:
                local_pred = self.local_rfs[group].predict_proba(X[i:i+1])
                # Weight local predictions based on group performance
                weight = self.group_performances[group]['auc']
                predictions[i] = (global_preds[i] * (1 - weight) +
                                local_pred[0] * weight)
            else:
                predictions[i] = global_preds[i]

        return predictions

def analyze_area_specific_performance(df, severity_type):
    """
    Analyze crash patterns specifically by area
    """
    print(f"\nAnalyzing {severity_type} crashes by area:")
    print("=" * 50)

    # Prepare data as before
    severity_mapping = {
        'No Apparent Injury': 0,
        'Possible Injury': 1,
        'Suspected Minor Injury': 1,
        'Suspected Serious Injury': 2,
        'Fatal Injury (Killed)': 2
    }

    df['severity_score'] = df['Crash_Severity'].map(severity_mapping)
    df['is_no_injury'] = (df['severity_score'] == 0).astype(int)
    df['is_moderate'] = (df['severity_score'] == 1).astype(int)
    df['is_severe'] = (df['severity_score'] == 2).astype(int)

    target = f'is_{severity_type}' if severity_type != 'severe' else 'is_severe'

    # Analyze each area separately
    area_results = {}
    for area_type in ['rural', 'urban', 'suburban']:
        print(f"\nAnalyzing {area_type} areas:")

        # Filter for current area
        area_df = df[df['area'] == area_type].copy()

        # Balance dataset within this area
        positive_cases = area_df[area_df[target] == 1]
        negative_cases = area_df[area_df[target] == 0]
        sample_size = min(len(positive_cases), len(negative_cases))

        if len(positive_cases) > sample_size:
            positive_cases = positive_cases.sample(n=sample_size, random_state=42)
        if len(negative_cases) > sample_size:
            negative_cases = negative_cases.sample(n=sample_size, random_state=42)

        area_balanced_df = pd.concat([positive_cases, negative_cases]).reset_index(drop=True)

        # Prepare features for this area
        X = prepare_predictors(area_balanced_df)
        y = area_balanced_df[target]

        # Train Random Forest for this area
        rf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)

        # Cross-validation for this area
        kf = KFold(n_splits=5, shuffle=True, random_state=42)
        area_cv_scores = []
        area_feature_importance = []

        for train_idx, val_idx in kf.split(X):
            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]

            rf.fit(X_train, y_train)
            y_pred_prob = rf.predict_proba(X_val)[:, 1]

            metrics = {
                'Accuracy': accuracy_score(y_val, y_pred_prob > 0.5),
                'Precision': precision_score(y_val, y_pred_prob > 0.5),
                'Recall': recall_score(y_val, y_pred_prob > 0.5),
                'F1': f1_score(y_val, y_pred_prob > 0.5),
                'ROC_AUC': roc_auc_score(y_val, y_pred_prob)
            }
            area_cv_scores.append(metrics)
            area_feature_importance.append(rf.feature_importances_)

        # Store results for this area
        area_results[area_type] = {
            'metrics': pd.DataFrame(area_cv_scores).mean(),
            'metrics_std': pd.DataFrame(area_cv_scores).std(),
            'feature_importance': pd.DataFrame({
                'Feature': X.columns,
                'Importance': np.mean(area_feature_importance, axis=0)
            }).sort_values('Importance', ascending=False),
            'sample_size': len(area_balanced_df)
        }

        # Print area-specific results
        print(f"\nSample size: {len(area_balanced_df)}")
        print("\nAverage metrics:")
        print(area_results[area_type]['metrics'])
        print("\nTop 5 most important features:")
        print(area_results[area_type]['feature_importance'].head())

    return area_results

# Usage
df = pd.read_csv('new_test_data_oct_7.csv', low_memory=False)

for severity_type in ['no_injury', 'moderate', 'severe']:
    area_results = analyze_area_specific_performance(df, severity_type)

    # Compare areas
    print(f"\nComparison of areas for {severity_type} crashes:")
    print("=" * 50)
    metrics_comparison = pd.DataFrame({
        area: results['metrics']
        for area, results in area_results.items()
    })
    print("\nMetrics by area:")
    print(metrics_comparison)

    # Feature importance comparison
    print("\nTop 3 features by area:")
    for area, results in area_results.items():
        print(f"\n{area.title()}:")
        print(results['feature_importance'].head(10))

pip install cramer_v

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import chi2_contingency
from sklearn.metrics import mutual_info_score

def analyze_categorical_relationship(df, col1, col2):
    """
    Analyze relationship between two categorical variables using multiple methods
    """
    print(f"\nAnalyzing relationship between {col1} and {col2}")
    print("=" * 50)

    # Create contingency table
    contingency = pd.crosstab(df[col1], df[col2])

    # Chi-square test
    chi2, p_value, dof, expected = chi2_contingency(contingency)

    # Calculate Cramer's V manually
    n = len(df)
    min_dim = min(len(df[col1].unique()), len(df[col2].unique()))
    cramer_v = np.sqrt(chi2 / (n * (min_dim - 1)))

    # Mutual Information
    mi = mutual_info_score(df[col1], df[col2])

    print("\nStatistical Tests:")
    print(f"Chi-square statistic: {chi2:.2f}")
    print(f"P-value: {p_value:.2e}")
    print(f"Cramer's V: {cramer_v:.3f}")
    print(f"Mutual Information: {mi:.3f}")

    # Value counts for each variable
    print("\nUnique values in each variable:")
    print(f"\n{col1} unique values: {len(df[col1].unique())}")
    print(f"{col2} unique values: {len(df[col2].unique())}")

    # Most common combinations
    print("\nTop 10 most common combinations:")
    combinations = df.groupby([col1, col2]).size().reset_index()
    combinations.columns = [col1, col2, 'Count']
    combinations['Percentage'] = combinations['Count'] / len(df) * 100
    print(combinations.nlargest(10, 'Count'))

    # Create correlation plot
    plt.figure(figsize=(12, 8))
    contingency_norm = contingency.div(contingency.sum(axis=1), axis=0)
    sns.heatmap(contingency_norm, cmap='YlOrRd', annot=False)  # Set annot=False for better readability
    plt.title(f'Normalized Contingency Table: {col1} vs {col2}')
    plt.xlabel(col2)
    plt.ylabel(col1)
    plt.xticks(rotation=90)
    plt.yticks(rotation=0)
    plt.tight_layout()
    plt.show()

    # Calculate conditional probabilities
    print("\nTop conditional probabilities:")
    cond_prob = contingency_norm.unstack()
    print("\nTop 10 highest conditional probabilities:")
    print(cond_prob.nlargest(10))

    return {
        'chi2': chi2,
        'p_value': p_value,
        'cramer_v': cramer_v,
        'mutual_info': mi,
        'contingency': contingency,
        'conditional_prob': cond_prob
    }

def analyze_frequency_correlation(df, col1_freq, col2_freq):
    """
    Analyze correlation between frequency-encoded versions of variables
    """
    correlation = df[col1_freq].corr(df[col2_freq])

    plt.figure(figsize=(8, 6))
    plt.scatter(df[col1_freq], df[col2_freq], alpha=0.5)
    plt.xlabel(col1_freq)
    plt.ylabel(col2_freq)
    plt.title(f'Scatter Plot of Frequency-Encoded Variables\nCorrelation: {correlation:.3f}')
    plt.tight_layout()
    plt.show()

    # Additional analysis of the frequency distributions
    print("\nFrequency encoding statistics:")
    print("\nCorrelation coefficient:", correlation)
    print(f"\n{col1_freq} distribution:")
    print(df[col1_freq].describe())
    print(f"\n{col2_freq} distribution:")
    print(df[col2_freq].describe())

    return correlation

# Load and analyze data
df = pd.read_csv('new_test_data_oct_7.csv', low_memory=False)

# First analyze original categorical variables
results = analyze_categorical_relationship(df, 'Crash_Type', 'Causal_Unit_First_Event')

# Print value distributions
print("\nDetailed value distributions:")
print("\nTop 10 Crash Types:")
print(df['Crash_Type'].value_counts().nlargest(10))
print("\nTop 10 Causal Unit First Events:")
print(df['Causal_Unit_First_Event'].value_counts().nlargest(10))

# If you have frequency encoded versions, analyze those too
if 'Crash_Type_freq' in df.columns and 'Causal_Unit_First_Event_freq' in df.columns:
    print("\nAnalyzing frequency-encoded versions:")
    freq_correlation = analyze_frequency_correlation(df,
                                                  'Crash_Type_freq',
                                                  'Causal_Unit_First_Event_freq')

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import re

# Load the dataset
df = pd.read_csv('new_test_data_oct_7.csv', dtype=str, low_memory=False)

column1_name = 'Crash_Type'  # replace with your actual column name
column2_name = 'Causal_Unit_First_Event'  # replace with your actual column name

# Text cleaning function
def clean_text(text):
    text = str(text).lower()  # Convert to lowercase
    text = re.sub(r'\W+', ' ', text)  # Remove punctuation
    return text

# Apply text cleaning to both columns
df[column1_name] = df[column1_name].fillna("").apply(clean_text)
df[column2_name] = df[column2_name].fillna("").apply(clean_text)

def calculate_similarity(df, col1, col2):
    combined_data = df[col1].tolist() + df[col2].tolist()

    vectorizer = TfidfVectorizer()
    tfidf_matrix = vectorizer.fit_transform(combined_data)

    num_rows = df.shape[0]
    similarities = []
    for i in range(num_rows):
        similarity = cosine_similarity(tfidf_matrix[i], tfidf_matrix[i + num_rows])
        similarities.append(similarity[0][0])

    df['Similarity'] = similarities
    return df

# Calculate and display the similarity
df = calculate_similarity(df, column1_name, column2_name)
print(df[['Similarity']])

# Display a few examples with zero similarity
print("Examples with zero similarity:")
print(df[df['Similarity'] == 0][[column1_name, column2_name]].head())

# Display a few examples with non-zero similarity
print("\nExamples with non-zero similarity:")
print(df[df['Similarity'] > 0][[column1_name, column2_name, 'Similarity']].head())

# Combine both aggregated columns for vectorization
combined_text = ['Crash_Type', 'Causal_Unit_First_Event']

# Initialize TF-IDF Vectorizer and compute the TF-IDF matrix
vectorizer = TfidfVectorizer()
tfidf_matrix = vectorizer.fit_transform(combined_text)

# Calculate cosine similarity between the two aggregated columns
similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]

print(f"Overall similarity between the two columns: {similarity:.4f}")

column1_name = 'Crash_Type'  # replace with your actual column name
column2_name = 'Causal_Unit_First_Event'  # replace with your actual column name